{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python [conda env:ndp]","language":"python","name":"conda-env-ndp-py"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.5"},"colab":{"name":"Day100_transfer_learning_HW.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"lpJK_janZBec"},"source":["## 作業\n","礙於不是所有同學都有 GPU ，這邊的範例使用的是簡化版本的 ResNet，確保所有同學都能夠順利訓練!\n","\n","\n","最後一天的作業請閱讀這篇非常詳盡的[文章](https://blog.gtwang.org/programming/keras-resnet-50-pre-trained-model-build-dogs-cats-image-classification-system/)，基本上已經涵蓋了所有訓練　CNN 常用的技巧，請使用所有學過的訓練技巧，盡可能地提高 Cifar-10 的 test data 準確率，截圖你最佳的結果並上傳來完成最後一次的作業吧!\n","\n","另外這些技巧在 Kaggle 上也會被許多人使用，更有人會開發一些新的技巧，例如使把預訓練在 ImageNet 上的模型當成 feature extractor 後，再拿擷取出的特徵重新訓練新的模型，這些技巧再進階的課程我們會在提到，有興趣的同學也可以[參考](https://www.kaggle.com/insaff/img-feature-extraction-with-pretrained-resnet)"]},{"cell_type":"code","metadata":{"id":"rw8-agZJ_X6g"},"source":["from keras.datasets import cifar10\n","from keras.applications.resnet import ResNet50 \n","from keras.models import Model\n","from keras.optimizers import Adam\n","from keras.utils import to_categorical\n","from keras.preprocessing.image import ImageDataGenerator\n","from keras.layers import Flatten, Dense, Dropout, Input, AveragePooling2D\n","\n","# 讀取資料集並作前處理\n","(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n","x_train = x_train.astype('float32')/ 255.\n","x_test = x_test.astype('float32')/ 255.\n","y_train = to_categorical(y_train, 10)\n","y_test = to_categorical(y_test, 10)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0zmNLlWf1W-d"},"source":["# write callback here\n","from keras.callbacks import LearningRateScheduler, TensorBoard, Callback, ReduceLROnPlateau\n","from IPython.display import clear_output\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","def lr_schedule(epoch):\n","    lr = 1e-3\n","    if epoch > 180:\n","        lr *= 0.5e-3\n","    elif epoch > 160:\n","        lr *= 1e-3\n","    elif epoch > 120:\n","        lr *= 1e-2\n","    elif epoch > 80:\n","        lr *= 1e-1\n","    print('Learning rate: ', lr)\n","    return lr\n","\n","class PlotLearning(Callback):\n","    def on_train_begin(self, logs={}):\n","        self.x = []\n","        self.losses = []\n","        self.val_losses = []\n","        self.acc = []\n","        self.val_acc = []\n","        self.lr = []\n","        self.fig = plt.figure()\n","        \n","        \n","        self.logs = []\n","\n","    def on_epoch_end(self, epoch, logs={}):\n","        \n","        self.logs.append(logs)\n","        self.x.append(epoch)\n","\n","        self.losses.append(logs.get('loss'))\n","        self.val_losses.append(logs.get('val_loss'))\n","        self.acc.append(logs.get('accuracy'))\n","        self.val_acc.append(logs.get('val_accuracy'))\n","        self.lr.append(logs.get('lr'))\n","\n","        f, (ax1, ax2, ax3) = plt.subplots(1, 3, sharex=True,figsize = (15,5))\n","        \n","        clear_output(wait=True)\n","        \n","        ax1.set_yscale('log')\n","        ax1.plot(self.x, self.losses, label=\"loss\")\n","        ax1.plot(self.x, self.val_losses, label=\"val_loss\")\n","        ax1.set_title('epoch_loss')\n","        ax1.set(xlabel='Epoch', ylabel='loss')\n","        ax1.legend()\n","\n","        ax2.plot(self.x, self.acc, label=\"acc\")\n","        ax2.plot(self.x, self.val_acc, label=\"val_acc\")\n","        ax2.set_title('epoch_accuracy')\n","        ax2.set(xlabel='Epoch', ylabel='accuracy')\n","        ax2.legend()\n","        \n","        ax3.plot(self.x, self.lr)\n","        ax3.set_title('Learning rate')\n","        ax3.set(xlabel='Epoch', ylabel='')\n","\n","        plt.show();\n","\n","class show_epoch(Callback):\n","    def on_epoch_end(self, epoch, logs={}): \n","        print('Epoch: ',epoch+1)\n","        \n","display_epoch = show_epoch()\n","\n","lr_scheduler = LearningRateScheduler(lr_schedule,\n","                                     verbose=0)\n","\n","lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1),\n","                               cooldown=0,\n","                               patience=5,\n","                               min_lr=0.5e-6)\n","\n","TensorBoardColabCallback = TensorBoard(log_dir='./logs',\n","                                       histogram_freq=0,\n","                                       write_graph=False,\n","                                       write_images=False,\n","                                       embeddings_freq=0,\n","                                       embeddings_layer_names=None,\n","                                       embeddings_metadata=None)\n","\n","plot_accval = PlotLearning()\n","\n","callbacks = [lr_scheduler, TensorBoardColabCallback]\n","\n","if TensorBoardColabCallback in callbacks:\n","    %load_ext tensorboard\n","    %tensorboard --logdir logs\n","else:\n","    print(\"Not using tensorboard.\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XJJmosWbDS_m"},"source":["# Hyperparamters\n","batch_size = 32 # batch 的大小，如果出現 OOM error，請降低這個值\n","num_classes = 10 # 類別的數量，Cifar 10 共有 10 個類別\n","epochs = 200 # 訓練整個資料集共 30個循環\n","data_augmentation = True"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xerMmXlTCt3e"},"source":["# Model build here\n","net = ResNet50(include_top=False, weights=None, input_shape=x_train.shape[1:], classes=num_classes)\n","x = Flatten()(net.output)\n","out = Dense(units=10, activation='softmax')(x)\n","model = Model(net.input, out)\n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zzXIo6NlZBed","outputId":"aaf3d28a-1eb9-4c20-975c-6f007524b684","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["# Training\n","model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n","\n","if not data_augmentation:\n","    print('Not using data augmentation.')\n","    model.fit(x_train, y_train,\n","              batch_size=batch_size,\n","              epochs=epochs,\n","              validation_data=(x_test, y_test),\n","              shuffle=True,\n","              callbacks=callbacks)\n","else:\n","    print('Using data augmentation.')\n","    # Data augmentation\n","    datagen = ImageDataGenerator(\n","        horizontal_flip=True,\n","        zca_epsilon=1e-06,\n","        width_shift_range=0.1,\n","        height_shift_range=0.1)\n","\n","    model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n","                        validation_data=(x_test, y_test),\n","                        epochs=epochs,\n","                        callbacks=callbacks,\n","                        verbose=2)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Using data augmentation.\n","WARNING:tensorflow:From <ipython-input-5-2256918def89>:25: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use Model.fit, which supports generators.\n","Learning rate:  0.001\n","Epoch 1/200\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n","Instructions for updating:\n","use `tf.profiler.experimental.stop` instead.\n","WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0759s vs `on_train_batch_end` time: 0.2362s). Check your callbacks.\n","1563/1563 - 204s - loss: 2.3217 - accuracy: 0.2796 - val_loss: 5.0117 - val_accuracy: 0.2388\n","Learning rate:  0.001\n","Epoch 2/200\n","1563/1563 - 202s - loss: 2.2327 - accuracy: 0.2774 - val_loss: 5.5346 - val_accuracy: 0.1870\n","Learning rate:  0.001\n","Epoch 3/200\n","1563/1563 - 201s - loss: 2.2458 - accuracy: 0.2645 - val_loss: 1.9642 - val_accuracy: 0.2966\n","Learning rate:  0.001\n","Epoch 4/200\n","1563/1563 - 202s - loss: 1.9170 - accuracy: 0.3513 - val_loss: 5.4953 - val_accuracy: 0.2410\n","Learning rate:  0.001\n","Epoch 5/200\n","1563/1563 - 201s - loss: 2.0800 - accuracy: 0.3114 - val_loss: 11.2669 - val_accuracy: 0.3236\n","Learning rate:  0.001\n","Epoch 6/200\n","1563/1563 - 202s - loss: 2.1368 - accuracy: 0.2761 - val_loss: 1.7202 - val_accuracy: 0.3851\n","Learning rate:  0.001\n","Epoch 7/200\n","1563/1563 - 201s - loss: 1.7765 - accuracy: 0.3883 - val_loss: 1.5336 - val_accuracy: 0.4455\n","Learning rate:  0.001\n","Epoch 8/200\n","1563/1563 - 202s - loss: 1.6473 - accuracy: 0.4345 - val_loss: 1.6333 - val_accuracy: 0.4496\n","Learning rate:  0.001\n","Epoch 9/200\n","1563/1563 - 202s - loss: 1.6388 - accuracy: 0.4411 - val_loss: 1.5910 - val_accuracy: 0.4889\n","Learning rate:  0.001\n","Epoch 10/200\n","1563/1563 - 202s - loss: 1.4544 - accuracy: 0.4986 - val_loss: 1.4055 - val_accuracy: 0.5111\n","Learning rate:  0.001\n","Epoch 11/200\n","1563/1563 - 202s - loss: 1.3512 - accuracy: 0.5292 - val_loss: 1.2224 - val_accuracy: 0.5591\n","Learning rate:  0.001\n","Epoch 12/200\n","1563/1563 - 202s - loss: 1.2821 - accuracy: 0.5536 - val_loss: 1.4692 - val_accuracy: 0.4721\n","Learning rate:  0.001\n","Epoch 13/200\n","1563/1563 - 201s - loss: 1.1851 - accuracy: 0.5831 - val_loss: 1.3723 - val_accuracy: 0.5707\n","Learning rate:  0.001\n","Epoch 14/200\n","1563/1563 - 202s - loss: 1.0699 - accuracy: 0.6266 - val_loss: 1.0080 - val_accuracy: 0.6448\n","Learning rate:  0.001\n","Epoch 15/200\n","1563/1563 - 202s - loss: 1.0064 - accuracy: 0.6480 - val_loss: 1.1409 - val_accuracy: 0.6040\n","Learning rate:  0.001\n","Epoch 16/200\n","1563/1563 - 202s - loss: 0.9455 - accuracy: 0.6704 - val_loss: 1.3605 - val_accuracy: 0.5467\n","Learning rate:  0.001\n","Epoch 17/200\n","1563/1563 - 202s - loss: 0.8891 - accuracy: 0.6884 - val_loss: 1.0625 - val_accuracy: 0.6408\n","Learning rate:  0.001\n","Epoch 18/200\n","1563/1563 - 202s - loss: 0.8674 - accuracy: 0.6973 - val_loss: 0.9496 - val_accuracy: 0.6751\n","Learning rate:  0.001\n","Epoch 19/200\n","1563/1563 - 202s - loss: 0.8027 - accuracy: 0.7194 - val_loss: 0.7909 - val_accuracy: 0.7227\n","Learning rate:  0.001\n","Epoch 20/200\n","1563/1563 - 203s - loss: 0.7645 - accuracy: 0.7326 - val_loss: 0.7757 - val_accuracy: 0.7326\n","Learning rate:  0.001\n","Epoch 21/200\n","1563/1563 - 202s - loss: 0.7360 - accuracy: 0.7437 - val_loss: 0.7966 - val_accuracy: 0.7246\n","Learning rate:  0.001\n","Epoch 22/200\n","1563/1563 - 203s - loss: 0.7218 - accuracy: 0.7498 - val_loss: 0.7873 - val_accuracy: 0.7292\n","Learning rate:  0.001\n","Epoch 23/200\n","1563/1563 - 202s - loss: 0.6831 - accuracy: 0.7639 - val_loss: 0.8072 - val_accuracy: 0.7220\n","Learning rate:  0.001\n","Epoch 24/200\n","1563/1563 - 202s - loss: 0.6612 - accuracy: 0.7690 - val_loss: 0.7370 - val_accuracy: 0.7509\n","Learning rate:  0.001\n","Epoch 25/200\n","1563/1563 - 202s - loss: 0.6399 - accuracy: 0.7761 - val_loss: 0.7090 - val_accuracy: 0.7502\n","Learning rate:  0.001\n","Epoch 26/200\n","1563/1563 - 202s - loss: 0.6177 - accuracy: 0.7845 - val_loss: 1.0469 - val_accuracy: 0.6607\n","Learning rate:  0.001\n","Epoch 27/200\n","1563/1563 - 202s - loss: 0.6418 - accuracy: 0.7788 - val_loss: 0.6568 - val_accuracy: 0.7742\n","Learning rate:  0.001\n","Epoch 28/200\n","1563/1563 - 202s - loss: 0.5864 - accuracy: 0.7953 - val_loss: 0.7123 - val_accuracy: 0.7546\n","Learning rate:  0.001\n","Epoch 29/200\n","1563/1563 - 203s - loss: 0.6171 - accuracy: 0.7871 - val_loss: 0.6996 - val_accuracy: 0.7633\n","Learning rate:  0.001\n","Epoch 30/200\n","1563/1563 - 203s - loss: 0.5571 - accuracy: 0.8070 - val_loss: 0.6110 - val_accuracy: 0.7881\n","Learning rate:  0.001\n","Epoch 31/200\n","1563/1563 - 202s - loss: 0.5725 - accuracy: 0.8001 - val_loss: 0.7470 - val_accuracy: 0.7504\n","Learning rate:  0.001\n","Epoch 32/200\n","1563/1563 - 202s - loss: 0.5395 - accuracy: 0.8138 - val_loss: 0.5821 - val_accuracy: 0.8003\n","Learning rate:  0.001\n","Epoch 33/200\n","1563/1563 - 202s - loss: 0.5190 - accuracy: 0.8206 - val_loss: 0.6890 - val_accuracy: 0.7685\n","Learning rate:  0.001\n","Epoch 34/200\n","1563/1563 - 203s - loss: 0.5111 - accuracy: 0.8223 - val_loss: 0.6137 - val_accuracy: 0.7866\n","Learning rate:  0.001\n","Epoch 35/200\n","1563/1563 - 202s - loss: 0.4840 - accuracy: 0.8311 - val_loss: 0.5745 - val_accuracy: 0.8051\n","Learning rate:  0.001\n","Epoch 36/200\n","1563/1563 - 202s - loss: 0.4921 - accuracy: 0.8301 - val_loss: 0.6399 - val_accuracy: 0.7898\n","Learning rate:  0.001\n","Epoch 37/200\n","1563/1563 - 202s - loss: 0.4666 - accuracy: 0.8384 - val_loss: 0.5872 - val_accuracy: 0.8039\n","Learning rate:  0.001\n","Epoch 38/200\n","1563/1563 - 202s - loss: 0.4682 - accuracy: 0.8378 - val_loss: 0.6980 - val_accuracy: 0.7643\n","Learning rate:  0.001\n","Epoch 39/200\n","1563/1563 - 202s - loss: 0.4502 - accuracy: 0.8444 - val_loss: 1.7092 - val_accuracy: 0.5222\n","Learning rate:  0.001\n","Epoch 40/200\n","1563/1563 - 202s - loss: 0.4516 - accuracy: 0.8421 - val_loss: 0.8688 - val_accuracy: 0.7292\n","Learning rate:  0.001\n","Epoch 41/200\n","1563/1563 - 203s - loss: 0.4175 - accuracy: 0.8549 - val_loss: 0.5987 - val_accuracy: 0.8042\n","Learning rate:  0.001\n","Epoch 42/200\n","1563/1563 - 203s - loss: 0.4153 - accuracy: 0.8536 - val_loss: 0.6089 - val_accuracy: 0.7999\n","Learning rate:  0.001\n","Epoch 43/200\n","1563/1563 - 202s - loss: 0.4090 - accuracy: 0.8566 - val_loss: 0.5918 - val_accuracy: 0.8020\n","Learning rate:  0.001\n","Epoch 44/200\n","1563/1563 - 202s - loss: 0.4395 - accuracy: 0.8450 - val_loss: 0.6594 - val_accuracy: 0.7989\n","Learning rate:  0.001\n","Epoch 45/200\n","1563/1563 - 202s - loss: 0.4094 - accuracy: 0.8580 - val_loss: 1.1103 - val_accuracy: 0.6592\n","Learning rate:  0.001\n","Epoch 46/200\n","1563/1563 - 203s - loss: 0.4414 - accuracy: 0.8466 - val_loss: 0.7494 - val_accuracy: 0.7929\n","Learning rate:  0.001\n","Epoch 47/200\n","1563/1563 - 202s - loss: 0.3766 - accuracy: 0.8692 - val_loss: 0.6448 - val_accuracy: 0.7951\n","Learning rate:  0.001\n","Epoch 48/200\n","1563/1563 - 203s - loss: 0.3714 - accuracy: 0.8703 - val_loss: 0.8144 - val_accuracy: 0.7522\n","Learning rate:  0.001\n","Epoch 49/200\n","1563/1563 - 202s - loss: 0.3712 - accuracy: 0.8709 - val_loss: 0.8467 - val_accuracy: 0.7441\n","Learning rate:  0.001\n","Epoch 50/200\n","1563/1563 - 203s - loss: 0.3497 - accuracy: 0.8774 - val_loss: 0.5800 - val_accuracy: 0.8148\n","Learning rate:  0.001\n","Epoch 51/200\n","1563/1563 - 202s - loss: 0.3383 - accuracy: 0.8799 - val_loss: 0.7511 - val_accuracy: 0.7753\n","Learning rate:  0.001\n","Epoch 52/200\n","1563/1563 - 203s - loss: 0.3383 - accuracy: 0.8828 - val_loss: 0.5528 - val_accuracy: 0.8204\n","Learning rate:  0.001\n","Epoch 53/200\n","1563/1563 - 203s - loss: 0.3507 - accuracy: 0.8781 - val_loss: 0.8267 - val_accuracy: 0.8052\n","Learning rate:  0.001\n","Epoch 54/200\n","1563/1563 - 203s - loss: 0.3133 - accuracy: 0.8908 - val_loss: 0.6280 - val_accuracy: 0.7990\n","Learning rate:  0.001\n","Epoch 55/200\n","1563/1563 - 203s - loss: 0.3519 - accuracy: 0.8779 - val_loss: 0.5724 - val_accuracy: 0.8190\n","Learning rate:  0.001\n","Epoch 56/200\n","1563/1563 - 203s - loss: 0.3064 - accuracy: 0.8921 - val_loss: 0.6089 - val_accuracy: 0.8135\n","Learning rate:  0.001\n","Epoch 57/200\n","1563/1563 - 202s - loss: 0.3153 - accuracy: 0.8886 - val_loss: 0.6817 - val_accuracy: 0.7973\n","Learning rate:  0.001\n","Epoch 58/200\n","1563/1563 - 203s - loss: 0.3290 - accuracy: 0.8860 - val_loss: 0.6402 - val_accuracy: 0.7996\n","Learning rate:  0.001\n","Epoch 59/200\n","1563/1563 - 202s - loss: 0.3021 - accuracy: 0.8958 - val_loss: 1.9797 - val_accuracy: 0.7951\n","Learning rate:  0.001\n","Epoch 60/200\n","1563/1563 - 203s - loss: 0.3483 - accuracy: 0.8794 - val_loss: 1.3249 - val_accuracy: 0.7936\n","Learning rate:  0.001\n","Epoch 61/200\n","1563/1563 - 202s - loss: 0.3298 - accuracy: 0.8864 - val_loss: 0.6610 - val_accuracy: 0.7981\n","Learning rate:  0.001\n","Epoch 62/200\n","1563/1563 - 203s - loss: 0.3124 - accuracy: 0.8903 - val_loss: 0.8434 - val_accuracy: 0.7514\n","Learning rate:  0.001\n","Epoch 63/200\n","1563/1563 - 203s - loss: 0.2793 - accuracy: 0.9013 - val_loss: 0.6206 - val_accuracy: 0.8163\n","Learning rate:  0.001\n","Epoch 64/200\n","1563/1563 - 203s - loss: 0.3025 - accuracy: 0.8952 - val_loss: 0.5740 - val_accuracy: 0.8222\n","Learning rate:  0.001\n","Epoch 65/200\n","1563/1563 - 203s - loss: 0.2723 - accuracy: 0.9043 - val_loss: 0.6821 - val_accuracy: 0.8038\n","Learning rate:  0.001\n","Epoch 66/200\n","1563/1563 - 203s - loss: 0.2754 - accuracy: 0.9044 - val_loss: 0.7465 - val_accuracy: 0.8184\n","Learning rate:  0.001\n","Epoch 67/200\n","1563/1563 - 203s - loss: 0.2620 - accuracy: 0.9106 - val_loss: 0.5860 - val_accuracy: 0.8246\n","Learning rate:  0.001\n","Epoch 68/200\n","1563/1563 - 203s - loss: 0.2536 - accuracy: 0.9116 - val_loss: 0.7543 - val_accuracy: 0.7853\n","Learning rate:  0.001\n","Epoch 69/200\n","1563/1563 - 203s - loss: 0.2547 - accuracy: 0.9119 - val_loss: 1.3879 - val_accuracy: 0.7731\n","Learning rate:  0.001\n","Epoch 70/200\n","1563/1563 - 203s - loss: 0.2562 - accuracy: 0.9097 - val_loss: 0.6109 - val_accuracy: 0.8244\n","Learning rate:  0.001\n","Epoch 71/200\n","1563/1563 - 203s - loss: 0.2382 - accuracy: 0.9161 - val_loss: 0.6500 - val_accuracy: 0.8104\n","Learning rate:  0.001\n","Epoch 72/200\n","1563/1563 - 203s - loss: 0.2402 - accuracy: 0.9159 - val_loss: 0.6450 - val_accuracy: 0.8122\n","Learning rate:  0.001\n","Epoch 73/200\n","1563/1563 - 202s - loss: 0.2439 - accuracy: 0.9170 - val_loss: 1.5657 - val_accuracy: 0.7836\n","Learning rate:  0.001\n","Epoch 74/200\n","1563/1563 - 203s - loss: 0.2561 - accuracy: 0.9101 - val_loss: 0.6428 - val_accuracy: 0.8169\n","Learning rate:  0.001\n","Epoch 75/200\n","1563/1563 - 203s - loss: 0.2167 - accuracy: 0.9238 - val_loss: 1.1672 - val_accuracy: 0.7815\n","Learning rate:  0.001\n","Epoch 76/200\n","1563/1563 - 203s - loss: 0.2923 - accuracy: 0.8992 - val_loss: 4.3314 - val_accuracy: 0.8086\n","Learning rate:  0.001\n","Epoch 77/200\n","1563/1563 - 203s - loss: 0.2527 - accuracy: 0.9119 - val_loss: 2.9278 - val_accuracy: 0.8029\n","Learning rate:  0.001\n","Epoch 78/200\n"],"name":"stdout"}]}]}